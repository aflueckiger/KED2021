---
title: The ABC of computational Text Analysis
subtitle: "07: Data Sources and Ethics"
author: "Alex Flückiger"
institute: |
  | Faculty of Humanities and Social Sciences
  | University of Lucerne
date: "9 April 2020"
lang: en-US
---



## Outline

- deepen the understanding of regex
- learn about available data resources
- think about ethics + bias &rarr; next session



::: notes

- Audio funktioniert + Flackern Bildschirm weg
- 3 Blöcke, Kursgestaltung noch offen

:::

## Ready for some interaction?

### Join the fun :iphone:

access with a code **51 63 60**: [https://www.menti.com](https://www.menti.com) 

access directly: [https://www.menti.com/sqs8kqsf6g](https://www.menti.com/sqs8kqsf6g)



::: notes

- Smartphone nutzen parallel zu Zoom
- Fragen weiterhin über Chat/Audio
- Standortbestimmung 
- Gestaltung heutige Sitzung

:::

## {data-background-iframe=https://www.mentimeter.com/embed/112d65f11d4dead10673437e52279113/39cfafd03655}

## {data-background-iframe=https://www.mentimeter.com/embed/112d65f11d4dead10673437e52279113/ae68b52b0d04}

## Recap last Lecture

- regex to extract + replace particular text
- match text with generalized patterns 
- 2 types of symbols
  - literal: `abc`
  - meta: `\w \s [^abc] *`



::: notes

- Regex für Extraktion + Säubern
  - man muss nur ungefähr wissen wonach suchen
  - generalisierte Form = Muster
- Literale = Zeichen steht für tatsächliches Zeichen (buchstabentreu)
- Meta-Zeichen = Zeichen mit spezieller Bedeutung
- Probleme bei Mac-User --> OLAT

:::



## Replacing + Removing

#### stream editor (sed)

- advanced find + replace using regex
  - `sed "s/WHAT/WITH/g" FILE`
- `sed` replaces any sequence,  `tr` only single symbols

```Bash
echo "hello" | sed "s/llo/y/g"		# replace "llo" with a "y"

# by setting the g flag in "s/llo/y/g",
# sed replaces all occurences, not only the first one
```



::: notes

- egrep für Extraktion, sed für Manipulation
  - wichtig um Daten aufzubereiten
- wie Funktion von Word, nur mächtiger dank Regex
- Löschen = Ersetzen mit leeren Sequenz
- flag "global"
- Demonstration mit \b
- echo `"hello hell" | sed "s/l\b/y/g"`

:::



## Contextual Replacing 

- reuse match with grouping `(pattern)`
  - `\1`  equals the expression inside first pair of parentheses
  - `\2`  expression of second pair
  - ...

```bash
# swap order of name (last first -> first last)
echo "Lastname Firstname" | sed -E "s/(.*) (.*)/\2 \1/"

# egrep also supports grouping
# match any two digit pair
egrep -r "([0-9])\1([0-9])\2"
```



::: notes

- Teilausdruck zu gruppieren zur Wiederverwendung
- Klammern 
  - ebenfalls Metazeichen

:::

## Even more Symbols

* `\b` word boundary

  * `word\b` does not match `words`

* `^` begin of line and `$` end of line

  * `^A` matches only `A` at line start

* `|` disjunction (OR)
  * `(Mr|Mrs|Mr\.|Mrs\.) Green` matches alternatives 




::: notes

- diese Symbole sind leer, sie matchen keine Zeichen
- spezifizieren Positon von regulärem Ausdruck
- line start hilfreich für übung

:::



## Something actually useful

#### combining regular expressions with frequency analysis

```bash
# get political areas 
# by counting words ending with "politik"
egrep -rioh '\w*politik\b' */*.txt | sort | uniq -c | sort -h

# get ideologies/concepts 
# by counting words ending with -ismus
egrep -rioh '\w*ismus\b' */*.txt | sort | uniq -c | sort -h
```



::: notes

- 

:::



## Greediness Trap

- greedy ~ match the longest string possible
- quantifiers `*` or `+` are greedy
- non-greedy by excluding some symbols
  -  `[^EXCLUDE_SYMBOLS]` instead of `.*`



```bash
# greedy: an apple, other apple
echo 'an apple, other apple' | egrep 'a.*apple'

# non-greedy: an apple
echo 'an apple, other apple' | egrep 'a[^,]*apple'
```

::: notes

- `. =` jegliche Zeichen, beliebige Länge



:::

## Better Tokenization

- tokenization ~ splitting into words

```bash
# new, improved approach
cat text.txt | tr -sc "[a-zäöüA-ZÄÖÜ0-9-]" "\n"

# old approach
cat text.txt | tr ' ' '\n'	
```

::: notes

- Tokenisierung = in Wörter splitten
- Interpunktion "klebt nicht mehr an Wörtern"
- -s = beliebig viele Zeichen
- -c = Komplement (also nicht diese Zeichen)
- angegebene Zeichen werden NICHT ersetzt

:::

## Questions

## In-class: Game {data-background=#3c70b5}

1. Make sure that your local copy of the Github repository KED2020 is up-to-date with `git pull`.  Go to the party programmes in `materials/party_programmes/txt`.
2. Use `egrep` to extract all uppercased words like `UNO, OECD, SP` and count their frequency.
3. Post the most frequent abbreviation from 2)  on [www.menti.com](www.menti.com).



```bash
# Some not so random hints 
piping with |
sort
uniq -c
egrep -roh */*.txt
```

::: notes

- `egrep -roh '[A-Z]{2,}' */*.txt | sort | uniq -c | sort -h `

:::

## {data-background-iframe=https://www.mentimeter.com/embed/112d65f11d4dead10673437e52279113/762fc5476a3a}

## {data-background-iframe=https://www.mentimeter.com/embed/112d65f11d4dead10673437e52279113/0bdd6cae3a6e}

## In-class: Exercises I{data-background=#3c70b5}

1. Use `egrep` to extract words following any of these strings: `der die das`.

2. Do the self-check on the next slide.

3. Use `sed -E` to remove the table of content, the footer and the page number in the programme of the Green Party. Check the PDF to get a visual impression and test your regular expression with `egrep` to see if you match the correct parts in the document.

   


## In-class: Self-Check{data-background=#3c70b5}

#### equivalent patterns

```bash
a+ == aa* 				# "a" once or more than once
a? == (a|_) 			# "a" once or nothing
a{3} == aaa				# three "a"
a{2,3} == (aa|aaa)		# two or three "a"
[ab] == (a|b)			# "a" or "b"
[0-9] == (0|1|2|3|4|5|6|7|8|9)	#any digit
```

## In-class: Exercise II{data-background=#3c70b5}

1. Since you know about RegEx, we can use a more sophisticated tokenizer to split a text into words. What  is the difference between the old and new approach?  Test it and check the helper page with `man`.

   ```bash
   # new, improved approach
   cat text.txt | tr -sc "[a-zäöüA-ZÄÖÜ0-9-]" "\n"
   # old approach
   cat text.txt | tr ' ' '\n'	
   ```

## In-class: Exercise III{data-background=#3c70b5}

1. Count all the bigrams (sequence of two words) using character sets and quantifiers. What about trigrams (three words)?
2. Extract the words following numbers (also consider numbers like: `1'000, 1,000 or 5%`). Then, count all the words while excluding the numbers themselves. Hint: Pipe another grep to remove the digits.
3. You are ready to come up with your own patterns...



::: notes

- number task
  - `egrep -rho "[0-9',%] \w+" | egrep -hoi '[a-z]+' | sort | uniq -c | sort -h`

:::



# Data Sources

## Forms of Data

- content data
  - clean, plain text data
  - preferable `.txt`
- metadata ~ information about the actual data
  - publishing date, authors, source, version
  - preferable as `.csv` 



::: notes 

- Block über Datensätze für Mini-Projekt
- Metadaten für feinere und vergleichende Analysen
- Daten in unterschiedlichen Formaten

:::

## What data sources are there?

- broadly social
  - newspapers + magazines
  - websites + social media
  - reports by NGOs/GOs
- scientific
  - journals
- economic
  - business plans/reports
  - contracts 
  - patents
- any textual source...



## Interesting Publishers

* [Nexis Uni](http://www.nexisuni.com/)
  * newspaper, business + legal reports (international)
  * licensed by the university
* [HathiTrust](https://www.hathitrust.org/datasets)
  * massive collection of books (international)
  * open, requires agreement
* [Project Gutenberg](https://www.gutenberg.org/)
  * huge collection of books (international)
  * open-access
* [JSTOR](https://www.jstor.org/dfr/)
  * scientific articles across disciplines
  * requires account, only 1-3-grams



::: notes

- Nexis vielleicht spannendste Quelle
  - ezproxy
  - download docx --> Umwandlung in text
- Wieso Literatur?
  - genderspezifische Sprache, Verweise Natur/Umwelt
- JSTOR
  - 

:::



## Dataset Search

* [Harvard Dataverse](https://dataverse.harvard.edu/)
  * free scientific data repository
* [Google Dataset Search](https://datasetsearch.research.google.com)
  * Google for datasets basically
* corpora by the [Department of Computational Linguistics @ UZH](https://www.cl.uzh.ch/en/texttechnologies/research/others.html)



<br>

:nerd_face: search for your topic followed by *corpus* or *text collection*



::: notes

* Suchmaschinen für Datensätze
* allerlei Datensätze, primär aus Wissenschaft



* Credit Suisse PDF Bulletin Corpus
* Boolean searches


:::



## Some great historical Corpora

#### ready off the shelf, machine-readable

- [1 August speeches by Swiss Federal Councillors](https://www.republik.ch/2019/08/01/anleitung-fuer-die-perfekte-ansprache-zum-1-august)
  - ask me for access to this corpus
- [Human Rights Texts](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/IAH8OY)
- [United Nations General Debate Corpus](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y)



## Online Computational Text Analysis

* [Impresso](https://impresso-project.ch/app/)
  * many historical newspapers + magazines (LU, CH)
  * requires account
* [bookworm HathiTrust](https://bookworm.htrc.illinois.edu)
  * great filtering by metadata
  * credible scientific source
* [Google Ngram Viewer](https://books.google.com/ngrams#)
  * no filtering option
  * useful for quick analysis

::: notes

- Datenanalysen online durchführen
- Absicherung über andere Quellen
- Impresso: Complete re-digitisation of the NZZ (together with the Zurich Central Library and Swiss National Library)

:::

## Copyright

- restricted access to high quality data :no_entry_sign:
- check the rights to process the data



::: notes

- open data unterschiedlich unterstützt
- Daten oftmals Teil von Geschäftsmodell
- nutzt Graubereich

:::

## ToDo: Next steps

- submit exercise #2
- check out the data resources
- think about the mini-project
  - groups
  - data



## Have a nice <br>Easter break! {data-background=../images/easter-eggs.jpg}

## References