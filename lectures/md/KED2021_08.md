---
title: The ABC of computational Text Analysis
subtitle: "08: Create your own Data Sets + Ethics"
author: "Alex Flückiger"
institute: |
  | Faculty of Humanities and Social Sciences
  | University of Lucerne
date: "23 April 2020"
lang: en-US
---



## Recap last Lecture

- cleaning with regular expression
- finding data sources



::: notes

- Osterpause
- Regex primär fürs säubern/preprocessing hilfreich
- interessante Quellen für Sozialwissenschaften

:::

## Outline

- feedback assignment #2
- use your texts as data :white_check_mark:
  - anything
  - from anytime
  - from anywhere
- care about ethics :see_no_evil::hear_no_evil::speak_no_evil: 

::: notes

- dreiteilige Sitzung
- nach heute gerüstet um mit (fast) allen Textdaten zu arbeiten
  - meistens Dokumente nicht in maschinenlesbarer Form

:::



# Assignment #2

## Feedback Assignment #2

[example solution](https://github.com/aflueckiger/KED2020/blob/master/exercises/exercise_2/flueckiger_KED2020_ex_2_solutions.sh)

- make patterns more general
  - date: `DD* Month DDDD`
- keep it simple
  - name of month ~ any word ~ `\w+`
- avoid false positives with positional information 
  - start of line: `^`
- names are hard to extract
  - variation + inconsistency

<br>

:nerd_face: check the count of matches



::: notes

- posititv: gut gelöst, weniger Zeit gebraucht als Übung 1
- Ziel Patterns: allgemeine Struktur/Eigenschaften beschreiben 
- tradeoff: generalisierung vs. spezifizität
  - so generell wie möglich, so spezifisch wie nötig
- Checks
  - Zählen der matches, wenn Anzahl Metadaten bekannt
  - Differenz File vor und nach Bereinigung
- Fragen

:::

# Converting Documents

## {data-background=../images/file_types.jpg}

<!-- https://www.studio-24-7.com/file-formats-and-their-uses-explained/ -->

::: notes

- extrem viele File-Typen
- mühsam, aber es gibt einfache Tools für Umwandlung

:::



## A world for humans ...

***news, press releases, reports from organizations***



:::::::::::::: {.columns}

:::{.column width="50%"}

digital documents <br>`.pdf`, `.docx`, `.html`<br>

:arrow_down:

**convert to `.txt`**

:::

:::{.column width="50%"}

scans of (old) documents <br>`.pdf`, `.jpg`, `.png`<br>

:arrow_down:

**Optical Character Recognition (OCR) **

:::

:::::::::::::: 

:white_check_mark:

**machine-readable**



::: notes

- PDF ist Publikationsstandard
  - neue (digital) vs. alte (scans)
  - Kriterium: Suche möglich?
- anschliessend Schritte zur Umwandlung der wichtigsten Formate
- Keine Konzepte lernen, wie bei RegEx
  - nur welches Tool, für welche Umwandlung
  - mehr oder weniger copy-paste

:::





## Conversion of DOCX

#### use case: news articles from [Nexis](https://www.nexisuni.com/)

- `pandoc` to convert file formats
- download as single articles in `.docx` on Nexis

```bash
# convert docx to txt
pandoc file_in.docx -t plain -o file_out.txt

### Install first with
brew install pandoc 	# macOS
sudo apt install pandoc # Ubuntu
```



<!-- [](../images/nexis.png) -->

::: notes

- Nexis = News-Datenbank
  - freier Zugang ezproxy
  - kennen ezproxy alle?
- kann auch html konvertieren
  - pandoc slides/KED2020_01.html -t plain

:::

## Conversion of digital PDF

#### use case: [Swiss party programmes](https://visuals.manifesto-project.wzb.eu/mpdb-shiny/cmp_dashboard_dataset/)

```bash
# convert digital native pdf to txt
pdftotext -nopgbrk -eol unix file_in.pdf

### Install first with
brew install poppler 			# macOS
sudo apt install poppler-utils 	# Ubuntu
```



::: notes

- dieselben Parteiprogramme, die wir schon analysiert haben
- Layout kann Extraktion erschweren
  - Spalten/Tabelle
- Häufigkeitsanalysen von Wörter sind robust, Struktur egal

:::

## Optical Character Recognition (OCR)

:::::::::::::: {.columns}

:::{.column width="50%"}



- OCR ~ convert ../images into text
  - text from scans/../images
  - handwriting + Fraktur texts
- image quality is crucial
- open-source software: `tesseract`
  - language-specific models



:::

:::{.column width="50%"}

![example OCR [(Wikipedia)](https://de.wikipedia.org/wiki/Texterkennung)](../images/ocr.png)

:::

::::::::::::::





::: notes

- tatsächlicher Buchstabe, nicht nur Bild davon

- Zwischenschritt Verbesserung Kontrast, B/W

- technisch Deep-Learning, nicht weiter von Bedeutung

- auch von Handy möglich, ohne teure Programmen


:::



## Conversion of digitalized PDF

#### use-case: [historical party programmes](https://visuals.manifesto-project.wzb.eu/mpdb-shiny/cmp_dashboard_dataset/)

1. extract image from PDF + improve contrast
2. run optical character recognition (OCR) on the image

```bash
# convert scanned pdf to tiff, control quality with parameters
convert -density 300 -depth 8 -strip -background white -alpha off \
file_in.pdf temp.tiff

echo test \
t

# run OCR for German ("eng" for English, "fra" for French)
tesseract -l deu temp.tiff file_out


### Install first with
brew install imagemagick			# macOS
sudo apt install imagemagick-6.q16	# Ubuntu
```



::: notes

- Zwei Schritte: Bildumwandlung + OCR
- tesseract funktioniert für viele Bildformate
  - nicht direkt für PDF
- Beispiel: Kassenbon fotografieren & mit Regex parsen
  - Wirtschaftswissenschaften: indexierter Warenkorb

:::

## #LifeHack: Make a PDF searchable {data-background=#4d7e65}

#### use case: scanned book chapters

```bash
# output searchable pdf instead of txt
convert -density 300 -depth 8 -strip -background white -alpha off -compress group4 \
file_in.pdf temp.tiff
tesseract -l deu temp.tiff file_out pdf
```



::: notes

- Output als PDF statt Text
- für Suchen/Zitate rauskopieren
- convert hier mit Kompression, da PDFs zu gross werden ansonsten

:::





## Scraping PDF from Websites

#### use case: [Swiss voting booklet](https://www.bk.admin.ch/bk/de/home/dokumentation/abstimmungsbuechlein.html)

- `wget` to download any files from the internet

```bash
# get a single file
wget EXACT_URL

# get all linked pdf from a single webpage
wget --recursive --accept pdf -nH --cut-dirs=5 \
--ignore-case --wait 1 --level 1 --directory-prefix=data \
https://www.bk.admin.ch/bk/de/home/dokumentation/abstimmungsbuechlein.html

# --accept FORMAT_OF_YOUR_INTEREST
# --directory-prefix YOUR_OUTPUT_DIRECTORY
```



::: notes

- Download & Dokumentation von Quellen
- Haupt-URL angeben, wo PDF verlinkt sind
- Scraping von Blogs möglich über Python  (nicht Teil von Seminar)
- nicht auf alle Argumente eingehen

:::

## Example Sources

* [Party Programmes across Europe](https://visuals.manifesto-project.wzb.eu/mpdb-shiny/cmp_dashboard_dataset/)
* [Swiss voting booklets](https://www.bk.admin.ch/bk/de/home/dokumentation/abstimmungsbuechlein.html)
* [1 August speeches by Swiss Federal Councillors](https://www.admin.ch/gov/de/start/dokumentation/reden/ansprachen-zum-nationalfeiertag.html)
* [Nestlé Annual Reports](https://www.nestle.com/csv/performance/downloads)
* ... any organization of your interest :thumbsup:



## Foundation of Batch Processing

#### perform the same operation on many files

```bash
# loop over all txt files
for file in *.txt; do

	# indent all commands in loop with a tab

	# rename each file
	# e.g. a.txt -> new_a.txt
	mv $file new_$file

done
```



::: notes

- Batch Processing = gleiche Operation durchführen für alle Files
- Erklären von Loop/Schleife und Variable
  - Wildcard zur Selektion > Liste von Files > Variable
- for-loop wichtiges Programmierkonzept
- Tabulator fürs Einrücken

:::

## Perform Batch OCR from PDF

```bash
for FILEPATH in *.pdf; do
	# convert pdf to image
    convert -density 300 $FILEPATH -depth 8 -strip \
    -background white -alpha off temp.tiff
    
    # define output name (remove .pdf from input)
    OUTFILE=${FILEPATH%.pdf} 
    
    # perform OCR on the tiff image
    tesseract -l deu temp.tiff $OUTFILE
    
    # remove the intermediate tiff image
    rm temp.tiff

done
```



## Preprocessing → RegEx{data-background=#b5533c}

![](../images/clean_data.png)

::: notes

- Aufbereitung unterschiedlich aufwendig
- für schnelle Analyse nicht notwendig
- nun alles da für Mini-Project, ausser wenn Lösung in Python

:::

# Bias & Ethics

## Don't be a fool ... {data-background=#b5533c}

... be wise, think twice.



::: notes

- wichtiges Thema, über Seminar hinaus
  - Studien/Geschäftsmodelle, die daneben sind
  - Ungleichheit: Geschlecht, Ethnie, sozioökonomisch
  - z.B. Unterschiede auf eine natürliche Basis stellen
  
  - ohne Prozess/Struktur zu bedenken
  
- alle betroffen: automatische Vorselektion Bewerbungen

- bestenfalls: naiv, schlechtensfalls: anti-liberal/diskriminierend

- Ethik ist nicht abstrakt und gehört nicht nur in Philosophie

  - nicht Begriff ist wichtig, sondern Denkart
  - nachdenken über Ausgangslage + Konsequenzen

- besseres Verständnis = bessere Data Science

:::



## Data = Digital Traces

- collecting, curating, preserving traces &rarr; uncover patterns
- data don’t disclose anything, you can speak with it though



::: notes

- Was sind Daten?
  - Daten sind kein Abbild der Welt, nichts natürliches.
- Daten liegen nicht einfach herum, sondern gemacht (siehe Schritte)

:::



## Imperfect Data: A Tail of Bias

* data/archive holes

  * lost, uncollected
* noise in data

  * OCR errors, inconsistent spelling, non-content
* corpus curation

  * supposition that key-word indicates topic
* social context 

<br>

:arrow_right: solution: reflect + tackle



::: notes

- fehlende, rauschende, selektive & verzerrte Daten
- sozialer Kontext
  - z.B. Budgetkürzung oder Neuausrichtung --> Wegfall von Thema
  - Sicht von weisen Männern auf Thema
- non-content elements
  - Metadaten, Kopfzeilen etc.

:::



## {data-background=#4d7e65} 

> Raw data is an oxymoron.
> <br>[@Gitelman2013]



## Data vs. Capta

> Differences in the etymological roots of the terms data and capta make the distinction between          constructivist and realist approaches clear. *Capta* is <span style="color:#b5533c">**"taken"**</span> actively while *data* is assumed to be a <span style="color:#b5533c">**"given"**</span> able to be recorded and observed. From this distinction, a world of differences arises. Humanistic inquiry acknowledges the situated, partial, and constitutive character of knowledge production, the recognition that knowledge is constructed, *taken*, <span style="color:#b5533c">**not simply given as a natural representation**</span> of pre-existing fact.
>
> [@Drucker2011]



## Key Principles

- Who has a voice in your data?
  - social context
- bigger is not necessarily better
  - more vs. more diverse data

* clean your data thoroughly
  * noisy vs. clean data



## {data-background=../images/datahumanism.jpeg}

<!-- https://giorgialupi.com/data-humanism-my-manifesto-for-a-new-data-wold -->

## Data represents real life. {data-background=#4d7e65}



## In-class: Exercises I{data-background=#3c70b5}

1. Make sure that your local copy of the Github repository KED2020 is up-to-date with `git pull`.  Check out the data samples and scripts in `materials/`.
2. Install the missing tools with the commands given on the respective slides: `pandoc, imagemagick, poppler`
3. **Digest the commands. Test them. Check the resources. Ask questions. Think about your mini-project.**
4. Download one or all *cogito* issues (PDF files) from the [UniLu website](https://www.unilu.ch/magazin/service/pdf-und-archiv/#section=c67079).
5. `wget` is a powerful tool. Have a look at its arguments and search for more examples in tutorials.

## Resources

#### Make a more sophisticated script for PDF conversion

- Erick Peirson. 2015. Tutorial: Text Extraction and OCR with Tesseract and ImageMagick - Methods in Digital and Computational Humanities - DigInG Confluence. [online](https://diging.atlassian.net/wiki/spaces/DCH/pages/5275668/Tutorial+Text+Extraction+and+OCR+with+Tesseract+and+ImageMagick)



## References
