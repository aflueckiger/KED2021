---
title: The ABC of computational Text Analysis
subtitle: "05: Basic NLP with Command-line"
author: "Alex Flückiger"
institute: |
  | Faculty of Humanities and Social Sciences
  | University of Lucerne
date: "26 March 2020"
lang: en-US
---

## Recap last Lecture

- using shell for file handling
- complete assignment



::: notes

- Zoom Online-Seminar
  - Unmittelbarkeit fehlt
  - Tempo letztes Mal zu schnell
  - heute ähnliches Tempo, dafür mehr Zeit zum Üben
- Übungen ok?
- technische Fragen?
  - Pipe hat Probleme gemacht, heute üben

:::

## Outline



- corpus linguistic using the shell
  - counting, finding, comparing
- analyzing programmes of swiss parties



::: notes

- inhaltliche Zumutung, heute sehr viel interessanter
- Frequenzanalysen = Schweizer Taschenmesser
- Ziel: mehr Übungszeit
- Syntax nicht merken, Wichtiges werdet ihr schlussendlich erinnern

:::

## When Politics changes, <br>Language changes.

![historical development of Swiss party politics ([Tagesanzeiger](https://blog.tagesanzeiger.ch/datenblog/index.php/1791/wie-sich-die-svp-aus-dem-buergerblock-verabschiedet-hat))](../images/swiss_party_politics.gif){height=11cm}



::: notes

- Language must be changed to change politics

:::



## Starting Point

- each text as separate file :white_check_mark:
  - start with shell
- document collection as dataset
  - proceed with Python?



::: notes

- bei Datensatz Python praktischer
- cell in tsv/csv-file



:::





# <br>Counting Things{data-background=../images/counting_blackboard.jpg}

## Frequency Analysis



:::::::::::::: {.columns}

:::{.column width="60%"}

* frequency ~ measure of relevance
* bag of words approach
* simple
* powerful

:::

:::{.column width="40%"}

![text as a bag of words ([source](https://cdn02.plentymarkets.com/r3pmentklgg2/item/../images/1716/full/Magnetwoerter.jpg))](../images/word_magnets.jpg)



:::

::::::::::::::



::: notes

- Häufigkeit indiziert Form von Relevanz
- in Häufigkeitsanalyse sind Worte kontextlos
  - BoW
  - radikale Vereinfachung = grösster Vorteil = Nachteil
  - Kontrolle, was dahinter steht
- ähnlich wie Google Ngram, aber eigene Daten

:::

## Key Figures of Texts

```bash
wc *.txt	# count number of lines, words, characters
```



::: notes

- zuerst Charakterisierung Datenquelle, nicht nur Inhalt
- Zahlen einzelne Texte und aggregiert

:::

## Word Occurrences

#### show in context

```bash
grep -ir 'data'			# search in current folder, ignore case

# other options
# --colour to highlight pattern
# --context 2 to show 2 lines above/below
```



#### count words

```bash
grep -ic 'data' */*.txt # count in all txt-files, ignore case
```



::: notes

- options
  - ignore case
  - recursive / specific files
- Dateinamen als Filter benutzen
  - Quelle/Jahr
  - grep -ir ' daten' `*svp*.txt`
- wc als Alternative


:::



## <span style="color:#111">Piping with | </span> {data-background=../images/lego_stack.jpg}

<!-- https://www.pinterest.ch/pin/518265869592086618/ -->

::: notes

- Zweck letztes Mal unklar
- Pipe modulares Zusammenbauen von Commands
- "Leim" zum Übergeben von Resultaten
- Files als Zwischenprodukte umgehen

::::

## Frequencies for all Words

#### steps of algorithm

1. split text into one word per line (tokenize)
2. sort words
3. count how often each word appears



```bash
# piping steps to get word frequencies
cat text.txt | tr ' ' '\n' | sort | uniq -c | sort -h > wordfreq.txt

# explanation of individual steps
tr ' ' '\n' 	# replace spaces with newline 
sort			# sort lines alphanumerically
uniq -c			# count repeated lines
```



::: notes

- Zweck: Häufigkeiten aller Wörter
- kein direkter Befehl -> Kombinieren von Befehlen
- Befehle erklären
  - Zusammenfassen gleicher Zeilen mit uniq
- Aggregation extrem flexibel
  - anderer Text, alle Texte
- Frage an Klasse: häufigstes Wort SVP?

:::



## Word Frequencies

- absolute frequency
- relative frequency
  - rel frq = occurrences / total_words
  - independent of size
- statistical validation of variation
  - significance tests between corpora



::: notes

- Korpus = Textsammlung
- 

:::



## Convert Stats into Dataset

- convert to tsv
- useful for further processing
  - export to Excel

```bash
# convert word frequencies into tsv-file
# additional step: replace a sequence of spaces with a tabulator
cat text.txt | tr ' ' '\n' | sort | uniq -c  | sort -h | \
tr -s ' ' '\t'  > test.tsv	
```

::: notes

- alle Leerschläge durch Tabulator ersetzen
- relative frequency in Excel

:::

# Preprocessing <br>to refine Results

## Common Preprocessing

* lowercasing
* replace symbols
* join lines
* trimming header + footer
* splitting into multiple files
* using patterns to remove/extract parts :date:



::: notes

- Regex nächste Woche

:::



## Lowercasing

- reduce word forms

```bash
echo 'ÜBER' | tr "A-ZÄÖÜ" "a-zäöü"	# fold text to lowercase
```



::: notes

- Grossschreibung Satzanfang

:::

## Removing and Replacing Symbols

```bash
echo "3x3" | tr -d [:digit:]	# remove all digits	
cat text.txt | tr -d [:punct:]	# remove punctuation like .,:; 

tr 'Y' 'Z'						# replace any Y with Z
```



::: notes

- löscht alle Einzelzeichen in Text (keine Sequenzen)

::: 



## Standard Preprocessing

- save preprocessed documents

```bash
cat speech.txt | tr A-ZÄÖÜ a-zäöü | \
tr -d [:punct:] | tr -d [:digit:] > speech_clean.txt
```

::: notes

- Kleinschreibung , keine Interpunktion, keine Zahlen

:::

## Join Lines 

```bash
cat test.txt | tr -s '\n' ' '	# replace newlines with spaces
```



::: notes

- harte Zeilenumbrüche entfernen

:::

## Trimming

```bash
more -N text.txt		# show line numbers
sed '1,10d' text.txt 	# remove the first 10 lines
```

## Splitting

```bash
# splits file at every delimiter into a stand-alone file
csplit huge_text.txt  "/delimiter/" {*}    
```



## Check Differences between Files

#### sanity check after modification

```bash
# show differences side-by-side and only differing lines
diff -y --suppress-common-lines text_raw.txt text_proc.txt
```



## Where there is a Shell,<br>there is a Way :thumbsup: {data-background=#4d7e65} 

::: notes

- Zusammenfassung
  - Shell =  flexibles + mächtiges Werkzeug

:::

# Questions{data-background=#3c70b5}

## In-class: Getting ready {data-background=#3c70b5}

1. Update your local copy of the Github repository KED2020 with `git pull`. You find some party programmes (Grüne, SP, SVP) in `materials/party_programmes`. The programmes are provided in plain text which I have extracted from the official PDFs.
2. Fool around with individual commands to get a feeling of them before you proceed with the actual analysis.

## In-class: Analyzing Swiss Party Programmes I{data-background=#3c70b5}

1. Compare the absolute frequencies of single terms or multi-word expressions ...
   - across parties
   - historically within a party

   Use the file names as filter to get various aggregation of the word counts.

2. Pick terms of your interest and look at their contextual use by extracting relevant passages. Does the usage differ across parties or time?



**Share your insights with the class using [Etherpad](https://etherpad.wikimedia.org/p/KED2020).** 

## In-class: Analyzing Swiss Party Programmes II{data-background=#3c70b5}

1. Convert the word frequencies per party into a `tsv` dataset. Compute the relative word frequency from the absolute frequency using any spreadsheet software (e.g. Excel). Are your conclusions still valid after accounting for the size?
2. Can you refine the results with further preprocessing of the data?
3. Get the number of unique words rather than the total number of words of a document (piping).



**Pro Tip** :nerd_face:: Use `grep` to look up commands in the course slides

## Additional Resources

When you look for useful primers on Bash, consider the following resources:

- [Tutorial Basic Text Analysis by W. Turkel](https://williamjturkel.net/2013/06/15/basic-text-analysis-with-command-line-tools-in-linux/)
- [Tutorial Pattern Matching + KWIC by W. Turkel](https://williamjturkel.net/2013/06/20/pattern-matching-and-permuted-term-indexing-with-command-line-tools-in-linux/)

